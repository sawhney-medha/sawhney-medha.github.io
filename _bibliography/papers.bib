---
---
Buttons:
- `abbr`: Adds an abbreviation to the left of the entry. You can add links to these by creating a venue.yaml-file in the _data folder and adding entries that match.
- `abstract`: Adds an "Abs" button that expands a hidden text field when clicked to show the abstract text
- `arxiv`: Adds a link to the Arxiv website (Note: only add the arxiv identifier here - the link is generated automatically)
- `bibtex_show`: Adds a "Bib" button that expands a hidden text field with the full bibliography entry
- `html`: Inserts an "HTML" button redirecting to the user-specified link
- `pdf`: Adds a "PDF" button redirecting to a specified file (if a full link is not specified, the file will be assumed to be placed in the /assets/pdf/ directory)
- `supp`: Adds a "Supp" button to a specified file (if a full link is not specified, the file will be assumed to be placed in the /assets/pdf/ directory)
- `blog`: Adds a "Blog" button redirecting to the specified link
- `code`: Adds a "Code" button redirecting to the specified link
- `poster`: Adds a "Poster" button redirecting to a specified file (if a full link is not specified, the file will be assumed to be placed in the /assets/pdf/ directory)
- `video`: Adds a "Video" button redirecting to a specified file (if a full link is not specified, the file will be assumed to be placed in the /assets/video/ directory)
- `slides`: Adds a "Slides" button redirecting to a specified file (if a full link is not specified, the file will be assumed to be placed in the /assets/pdf/ directory)
- `website`: Adds a "Website" button redirecting to the specified link
- `altmetric`: Adds an [Altmetric](https://www.altmetric.com/) badge (Note: if DOI is provided just use `true`, otherwise only add the altmetric identifier here - the link is generated automatically)
- `dimensions`: Adds a [Dimensions](https://www.dimensions.ai/) badge (Note: if DOI or PMID is provided just use `true`, otherwise only add the Dimensions' identifier here - the link is generated automatically)
@string{aps = {American Physical Society,}}

@comment{
@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein, A. and Podolsky, B. and Rosen, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  selected={true}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905},
  category = {Preprint}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  category = {Book Chapters}
}}

@article {Padhi2022.10.24.513423,
	author = {Abinash Padhi and Arka Daw and Medha Sawhney and Maahi M. Talukder and Atharva Agashe and Sohan Kale and Anuj Karpatne and Amrinder S. Nain},
	title = {Deep Learning Enabled Label-free Cell Force Computation in Deformable Fibrous Environments},
	elocation-id = {2022.10.24.513423},
	year = {2022},
	doi = {10.1101/2022.10.24.513423},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Through force exertion, cells actively engage with their immediate fibrous extracellular matrix (ECM) environment, causing dynamic remodeling of the environment and influencing cellular shape and contractility changes in a feedforward loop. Controlling cell shapes and quantifying the force-driven dynamic reciprocal interactions in a label-free setting is vital to understand cell behavior in fibrous environments but currently unavailable. Here, we introduce a force measurement platform termed crosshatch nanonet force microscopy (cNFM) that reveals new insights into cell shape-force coupling. Using a suspended crosshatch network of fibers capable of recovering in vivo cell shapes, we utilize deep learning methods to circumvent the fiduciary fluorescent markers required to recognize fiber intersections. Our method provides high fidelity computer reconstruction of different fiber architectures by automatically translating phase-contrast time-lapse images into synthetic fluorescent images. An inverse problem based on the nonlinear mechanics of fiber networks is formulated to match the network deformation and deformed fiber shapes to estimate the forces. We reveal an order-of-magnitude force changes associated with cell shape changes during migration, forces during cell-cell interactions and force changes as single mesenchymal stem cells undergo differentiation. Overall, deep learning methods are employed in detecting and tracking highly compliant backgrounds to develop an automatic and label-free force measurement platform to describe cell shape-force coupling in fibrous environments that cells would likely interact with in vivo.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/10/25/2022.10.24.513423},
	eprint = {https://www.biorxiv.org/content/early/2022/10/25/2022.10.24.513423.full.pdf},
	journal = {bioRxiv},
        category = {Preprints},
        bibtex_show={true},
        biorxiv = {10.1101/2022.10.24.513423v1},
        altmetric = {true},
        dimension = {true},
        preview = {CFM.jpg},
        abbr={bioRxiv},
}

@unpublished {cvpr_workshop,
              author = {Medha Sawhney* and Bhas Karmarkar* and Eric Leaman and Arka Daw and Anuj Karpatne and Bahareh Behkam},
              title = {Detecting and Tracking Hard-to-Detect Bacteria in Dense Porous Backgrounds},
              year = {2023},
              venue = {CVPR},
              note = {Oral + Poster Presentation in CV4Animals Workshop at CVPR 2023},
              abstract = {Studying bacteria motility is crucial to understanding and controlling biomedical and ecological phenomena involving bacteria. Tracking bacteria in complex environments such as polysaccharides (agar) or protein (collagen) hydrogels is a challenging task due to the lack of visually distinguishable features between bacteria and surrounding environment, making state-of-the-art methods for tracking easily recognizable objects such as pedestrians and cars unsuitable for this application. We propose a novel pipeline for detecting and tracking bacteria in bright-field microscopy videos involving bacteria in complex backgrounds. Our pipeline uses motion-based features and combines multiple models for detecting bacteria of varying difficulty levels. We apply multiple filters to prune false positive detections, and then use the SORT tracking algorithm with interpolation in case of missing detections. Our results demonstrate that our pipeline can accurately track hard-to-detect bacteria, achieving a high precision and recall.},
              poster = {https://drive.google.com/file/d/1vLgjvb7ziv9PgKG6i_6-wmAd0SbllHCU/view},
              website = {https://www.cv4animals.com/home},
              preview = {cvpr.png},
              category = {Workshop Papers},
              slides = {https://docs.google.com/presentation/d/1vTVuzMHudBhFC8muAsHCSljzGY2GG0VY/edit?usp=sharing&ouid=110697798623030402492&rtpof=true&sd=true},
              abbr={CVPR},
}

@article{sawhney2023memtrack,
      title={MEMTRACK: A Deep Learning-Based Approach to Microrobot Tracking in Dense and Low-Contrast Environments}, 
      author={Medha Sawhney and Bhas Karmarkar and Eric J. Leaman and Arka Daw and Anuj Karpatne and Bahareh Behkam},
      year={2023},
      eprint={2310.09441},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      code={https://github.com/sawhney-medha/MEMTrack},
      abstract={Tracking microrobots is challenging, considering their minute size and high speed. As the field progresses towards developing microrobots for biomedical applications and conducting mechanistic studies in physiologically relevant media (e.g., collagen), this challenge is exacerbated by the dense surrounding environments with feature size and shape comparable to microrobots. Herein, we report Motion Enhanced Multi-level Tracker (MEMTrack), a robust pipeline for detecting and tracking microrobots using synthetic motion features, deep learning-based object detection, and a modified Simple Online and Real-time Tracking (SORT) algorithm with interpolation for tracking. Our object detection approach combines different models based on the object's motion pattern. We trained and validated our model using bacterial micro-motors in collagen (tissue phantom) and tested it in collagen and aqueous media. We demonstrate that MEMTrack accurately tracks even the most challenging bacteria missed by skilled human annotators, achieving precision and recall of 77% and 48% in collagen and 94% and 35% in liquid media, respectively. Moreover, we show that MEMTrack can quantify average bacteria speed with no statistically significant difference from the laboriously-produced manual tracking data. MEMTrack represents a significant contribution to microrobot localization and tracking, and opens the potential for vision-based deep learning approaches to microrobot control in dense and low-contrast settings. All source code for training and testing MEMTrack and reproducing the results of the paper have been made publicly available this https URL.},
      category = {Preprints},
      altmetric = {true},
      dimension = {true},
      journal = {arXiv},
      bibtex_show={true},
      URL = {https://arxiv.org/abs/2310.09441},
      arxiv={2310.09441},
      preview = {memtrack_architecture.png},
      abbr={ArXiv},
}

@article{msawhney2024memtrack,
author = {Sawhney, Medha and Karmarkar, Bhas and Leaman, Eric J. and Daw, Arka and Karpatne, Anuj and Behkam, Bahareh},
title = {Motion Enhanced Multi-Level Tracker (MEMTrack): A Deep Learning-Based Approach to Microrobot Tracking in Dense and Low-Contrast Environments},
journal = {Advanced Intelligent Systems},
pages = {2300590},
year = {2024},
keywords = {bacteria, biohybrid microrobotics, collagen, computer vision, machine learning, multiobject tracking, object detection},
doi = {https://doi.org/10.1002/aisy.202300590},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202300590},
website = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202300590},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202300590},
abstract = {Tracking microrobots is challenging due to their minute size and high speed. In biomedical applications, this challenge is exacerbated by the dense surrounding environments with feature sizes and shapes comparable to microrobots. Herein, Motion Enhanced Multi-level Tracker (MEMTrack) is introduced for detecting and tracking microrobots in dense and low-contrast environments. Informed by the physics of microrobot motion, synthetic motion features for deep learning-based object detection and a modified Simple Online and Real-time Tracking (SORT)algorithm with interpolation are used for tracking. MEMTrack is trained and tested using bacterial micromotors in collagen (tissue phantom), achieving precision and recall of 76% and 51%, respectively. Compared to the state-of-the-art baseline models, MEMTrack provides a minimum of 2.6-fold higher precision with a reasonably high recall. MEMTrack's generalizability to unseen (aqueous) media and its versatility in tracking microrobots of different shapes, sizes, and motion characteristics are shown. Finally, it is shown that MEMTrack localizes objects with a root-mean-square error of less than 1.84 μm and quantifies the average speed of all tested systems with no statistically significant difference from the laboriously produced manual tracking data. MEMTrack significantly advances microrobot localization and tracking in dense and low-contrast settings and can impact fundamental and translational microrobotic research.},
category = {Journal Publications},
altmetric = {true},
dimension = {true},
bibtex_show={true},
publisher={Wiley Online Library},
abbr={AISY 2024},
preview={memtrack.png},
}



@inproceedings{
maruf2024vlmbio,
title={{VLM}4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images},
author={M. Maruf and Arka Daw and Kazi Sajeed Mehrab and Harish Babu Manogaran and Abhilash Neog and Medha Sawhney and Mridul Khurana and James Balhoff and Yasin Bakis and Bahadir Altintas and Matthew J Thompson and Elizabeth G Campolongo and Josef Uyeda and Hilmar Lapp and Henry Bart and Paula Mabee and Yu Su and Wei-Lun Chao and Charles Stewart and Tanya Berger-Wolf and Wasila M Dahdul and Anuj Karpatne},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=VHa0XNjWj2},
preview={vlm4bio.jpg},
code={https://github.com/sammarfy/VLM4Bio},
arxiv={2408.16176},
abbr={NeurIPS 2024},
category={Conference Proceedings},
abstract={Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of 12 state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of 469K question-answer pairs involving 30K images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images.}
}

@inproceedings{
anonymous2025a,
title={A Unified Framework for Forward and Inverse Problems in Subsurface Imaging using Latent Space Translations},
author={Naveen Gupta* and Medha Sawhney* and Arka Daw and Youzuo Lin and Anuj Karpatne},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=yIlyHJdYV3},
selected={true},
abbr={ICLR 2025},
preview={openfwi.png},
selected={true},
code={https://github.com/KGML-lab/Genralized-Forward-Inverse-Framework-for-DL4SI},
arxiv={2410.11247},
category = {Conference Proceedings},
website={https://kgml-lab.github.io/projects/GFI-framework/},
abs={In subsurface imaging, learning the mapping from velocity maps to seismic waveforms (forward problem) and waveforms to velocity (inverse problem) is important for several applications. While traditional techniques for solving forward and inverse problems are computationally prohibitive, there is a growing interest to leverage recent advances in deep learning to learn the mapping between velocity maps and seismic waveform images directly from data. Despite the variety of architectures explored in previous works, several open questions still remain unanswered such as the effect of latent space sizes, the importance of manifold learning, the complexity of translation models, and the value of jointly solving forward and inverse problems. We propose a unified framework to systematically characterize prior research in this area termed the Generalized Forward-Inverse (GFI) framework, building on the assumption of manifolds and latent space translations. We show that GFI encompasses previous works in deep learning for subsurface imaging, which can be viewed as specific instantiations of GFI. We also propose two new model architectures within the framework of GFI: Latent U-Net and Invertible X-Net, leveraging the power of U-Nets for domain translation and the ability of IU-Nets to simultaneously learn forward and inverse translations, respectively. We show that our proposed models achieve state-of-the-art (SOTA) performance for forward and inverse problems on a wide range of synthetic datasets, and also investigate their zero-shot effectiveness on two real-world-like datasets.},
pdf={https://openreview.net/forum?id=yIlyHJdYV3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2025%2FConference%2FAuthors%23your-submissions)}
}
@comment{
@article{phyics_diffusion,
title={Beyond Loss Guidance: Using PDE Residuals as Spectral Attention in Diffusion Neural Operators},
author={Medha Sawhney and Abhilash Neog and Mridul Khurana and Anuj Karpatne},
year={2025},
journal={Under Review},
selected={true},
category = {Workshop Papers},
abbr={Under Review},
preview={prisma.png},
}
}

@article{dutta2025openworldscenegraph,
title={Open World Scene Graph Generation using Vision Language Models}, 
author={Amartya Dutta and Kazi Sajeed Mehrab* and Medha Sawhney* and Abhilash Neog and Mridul Khurana and Sepideh Fatemi and Aanish Pradhan and M. Maruf and Ismini Lourentzou and Arka Daw and Anuj Karpatne},
year={2025},
arxiv={2506.08189},
selected={true},
venue = {CVPR},
journal={CV in the Wild Workshop, CVPR},
note={CVPR 2025 Workshop (CV in the Wild)},
abbr={CVPR W 2025},
preview={owsgg.png},
url={https://arxiv.org/abs/2506.08189}, 
}
@article{ml4phy_workshop_diffusion,
title={Investigating PDE Residual Attentions in Frequency Space for Diffusion Neural Operators},
author={Medha Sawhney and Abhilash Neog and Mridul Khurana and Arka Daw and Anuj Karpatne},
year={2025},
journal={ML4Physics Workshop, NeurIPS},
venue={NeurIPS},
selected={true},
note={Oral + Poster Presentation at ML4Physics Workshop},
category = {Workshop Papers},
abbr={NeurIPS W 2025},
preview={ml4phy_physics_diffusion.png},
}

@article{icml_workshop,
title={Toward Scientific Foundation Models for Aquatic Ecosystems},
author={Abhilash Neog and Medha Sawhney and K.S. Mehrab and Sepideh Fatemi Khorasgani and Mary E. Lofton and Amartya Dutta and Aanish Pradhan and Bennett J. McAfee and Emma Marchisin and Robert Ladwig and Arka Daw and Cayelan C. Carey and Paul Hanson and Anuj Karpatne},
year={2025},
booktitle={Foundation Models for Structured Data Workshop at ICML 2025},
journal={Under Review},
venue={ICML},
selected={true},
note={ICML 2025 Workshop (Foundation Models for Structured Data)},
preview={LakeFM.png},
category={Workshop Papers},
abbr={ICML W 2025},
pdf={https://openreview.net/forum?id=hxMPNdhfIO},
}

@article{cvpr_workshop_diffusion,
title={Physics-guided Diffusion Neural Operators for Solving Forward and Inverse PDEs},
author={Medha Sawhney and Abhilash Neog and Mridul Khurana and Amartya Dutta and Arka Daw and Anuj Karpatne},
year={2025},
journal={CV4Science, CVPR},
venue={CVPR},
note={Oral + Poster Presentation at CV4Science Workshop},
category = {Workshop Papers},
abbr={CVPR W 2025},
preview={cvpr_physics_diffusion.png},
}


@article{neog2025investigatingmodelagnosticimputationfreeapproach,
      title={Investigating a Model-Agnostic and Imputation-Free Approach for Irregularly-Sampled Multivariate Time-Series Modeling}, 
      author={Abhilash Neog and Arka Daw and Sepideh Fatemi Khorasgani and Medha Sawhney and Aanish Pradhan and Mary E. Lofton and Bennett J. McAfee and Adrienne Breef-Pilz and Heather L. Wander and Dexter W Howard and Cayelan C. Carey and Paul Hanson and Anuj Karpatne},
      year={2025},
	  journal={Under Review},
      eprint={2502.15785},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.15785}, 
	  arxiv={2502.15785},
      abbr={Under Review},
	  preview={misstsm.jpg},
}

@article {Padhi2022.10.24.513423,
	author = {Abinash Padhi and Arka Daw and Medha Sawhney and Maahi M. Talukder and Atharva Agashe and Sohan Kale and Anuj Karpatne and Amrinder S. Nain},
	title = {Deep Learning Reveals How Cells Pull, Buckle, and Navigate Fibrous Environments},
	year = {2025},
	doi = {10.1101/2022.10.24.513423},
	publisher={National Academy of Sciences},
	abstract = {Through force exertion, cells actively engage with their immediate fibrous extracellular matrix (ECM) environment, causing dynamic remodeling of the environment and influencing cellular shape and contractility changes in a feedforward loop. Controlling cell shapes and quantifying the force-driven dynamic reciprocal interactions in a label-free setting is vital to understand cell behavior in fibrous environments but currently unavailable. Here, we introduce a force measurement platform termed crosshatch nanonet force microscopy (cNFM) that reveals new insights into cell shape-force coupling. Using a suspended crosshatch network of fibers capable of recovering in vivo cell shapes, we utilize deep learning methods to circumvent the fiduciary fluorescent markers required to recognize fiber intersections. Our method provides high fidelity computer reconstruction of different fiber architectures by automatically translating phase-contrast time-lapse images into synthetic fluorescent images. An inverse problem based on the nonlinear mechanics of fiber networks is formulated to match the network deformation and deformed fiber shapes to estimate the forces. We reveal an order-of-magnitude force changes associated with cell shape changes during migration, forces during cell-cell interactions and force changes as single mesenchymal stem cells undergo differentiation. Overall, deep learning methods are employed in detecting and tracking highly compliant backgrounds to develop an automatic and label-free force measurement platform to describe cell shape-force coupling in fibrous environments that cells would likely interact with in vivo.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/10/25/2022.10.24.513423},
	eprint = {https://www.biorxiv.org/content/early/2022/10/25/2022.10.24.513423.full.pdf},
	journal = {Proceedings of the national academy of sciences},
        altmetric = {true},
        dimension = {true},
        preview = {CFM.jpg},
        abbr={PNAS 2025},
}

